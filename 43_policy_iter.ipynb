{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Foundations of Computational Economics #43\n",
    "\n",
    "by Fedor Iskhakov, ANU\n",
    "\n",
    "<img src=\"_static/img/dag3logo.png\" style=\"width:256px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Solving DP problems with policy iterations\n",
    "\n",
    "<img src=\"_static/img/lab.png\" style=\"width:64px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"_static/img/youtube.png\" style=\"width:65px;\">\n",
    "\n",
    "[https://youtu.be/m9R_io8mQoI](https://youtu.be/m9R_io8mQoI)\n",
    "\n",
    "Description: Policy iterations solution method for infinite horizon dynamic models. Solving stochastic inventory management problem with policy iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Policy iterations\n",
    "\n",
    "Also known as **Howard policy improvement** algorithm\n",
    "\n",
    "Area of application: dynamic decision problems with\n",
    "\n",
    "- infinite horizon, discrete time  \n",
    "- particularly well suited for problems with finite state space (discrete states)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Main idea of policy iterations\n",
    "\n",
    "Break the search of the fixed point (of Bellman or Coleman-Reffett operators) into two steps:\n",
    "\n",
    "1. Policy evaluation: compute value function for fixed policy function  \n",
    "1. Policy improvement : find best policy for given value function  \n",
    "\n",
    "\n",
    "Iterative (back and forth) approach instead of jointly finding the value and policy functions in the fixed point problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### General form of Bellman equation\n",
    "\n",
    "$$\n",
    "V(\\text{state}) = \\max_{\\text{decisions}} \\big[ U(\\text{state},\\text{decision}) + \\beta \\mathbb{E}\\big\\{ V(\\text{next state})  \\big| \\text{state},\\text{decision} \\big\\} \\big]\n",
    "$$\n",
    "\n",
    "- $ V(\\text{state}) $ is **value function** = maximum attainable (discounted) expected reward/utility/payoff  \n",
    "- $ U(\\text{state},\\text{decision}) $ is per-period/flow/instantaneous reward/utility/payoff  \n",
    "- (*next state*) is the *stochastic* next period state resulting from current state and decision  \n",
    "- expectation $ \\mathbb{E}\\{\\cdot\\} $ is taken over the distribution of the next period state conditional on current state and decision  \n",
    "- $ \\beta $ is a discount factor to measure future rewards in terms of current ones  \n",
    "\n",
    "\n",
    "The optimal choices are revealed along the solution of the Bellman equation as decisions which solve the maximization problem in the right hand side!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Policy iterations step 1\n",
    "\n",
    "For fixed policy function (decisions) solve for function $ V(\\text{state}) $:\n",
    "\n",
    "$$\n",
    "V(\\text{state}) = U(\\text{state},\\text{decision}) + \\beta \\mathbb{E}\\big\\{ V(\\text{next state})  \\big| \\text{state},\\text{decision} \\big\\}\n",
    "$$\n",
    "\n",
    "- functional equation, although simpler than Bellman equation  \n",
    "- becomes non-linear system of equations with discrete or discretized states  \n",
    "- becomes *linear* system of equations in many applications where $ \\mathbb{E}\\{\\cdot\\} $ can be expressed as matrix multiplication\n",
    "  - Markov discrete choice problems (all randomness in transition probabilities, no interpolation)\n",
    "  - when quadrature integration and linear interpolation are used in discretized state spaces  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Policy iterations step 2\n",
    "\n",
    "For fixed value function $ V_0(\\text{state}) $ find the optimal policy while computing:\n",
    "\n",
    "$$\n",
    "V(\\text{state}) = \\max_{\\text{decisions}} \\big[ U(\\text{state},\\text{decision}) + \\beta \\mathbb{E}\\big\\{ V_0(\\text{next state})  \\big| \\text{state},\\text{decision} \\big\\} \\big]\n",
    "$$\n",
    "\n",
    "- standard evaluation of the Bellman operator  \n",
    "- any previously considered approaches for implementation of the Bellman operator are applicable  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Why policy iterations are useful?\n",
    "\n",
    "*Rate of convergence!*\n",
    "\n",
    "- known to be considerably faster than VFI and time iterations  \n",
    "- yet, step 1 subproblem is comparable in complexity to VFI  \n",
    "- so, efficient implementation is necessary to justify the use  \n",
    "- *exact theoretical properties on complexity of the algorithm are unknown for the general case*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Which problems can be solved efficiently by policy iterations?\n",
    "\n",
    "- want to simplify the policy evaluation step as much as possible!  (otherwise, functional equation of the same complexity as solved by VFI)  \n",
    "- best is to present the Bellman equation as a linear system (vectorization, linear algebra solver)  \n",
    "- discrete choice, discrete finite state space problems are best  \n",
    "- with other problems, it’s not completely straightforward, but not impossible  \n",
    "\n",
    "\n",
    "Question: can linear interpolation of the value function on a fixed grid be represented as matrix multiplication?  Gaussian quadrature?\n",
    "3-dim array algebra as in stochastic consumption-savings problem with discretized choices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### General policy iterations algorithm\n",
    "\n",
    "1. Initialize policy or value function (in the latter case reverse the order of the next two steps)  \n",
    "1. Compute the value of the current policy by solving the Bellman equation without max operation (assuming that current policy is optimal)  \n",
    "1. Re-compute the policy function by applying the Bellman operator to the found value function  \n",
    "1. Repeat until convergence in policy or value function space  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inventory dynamics model\n",
    "\n",
    "Recall the inventory management problem (video 37 and exercise 11)\n",
    "\n",
    "- discrete state, discrete choice, infinite horizon in discrete time  \n",
    "- choice: $ q\\ge 0 $ is the order of new inventory  \n",
    "- state: $ x\\ge 0 $ is inventory in the current time period  \n",
    "- state: $ d\\ge 0 $ is stochastic demand in the current time period  \n",
    "- payoff in each period is given by  \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\pi & = & p \\cdot \\text{sales} - r \\cdot \\text{storage} - c [\\text{if order is made}] \\\\\n",
    "& = & p \\min\\{x,d\\} - r (x - \\min\\{x,d\\} + q) - c \\mathbb{1}\\{q>0\\} \\\\\n",
    "& = & p \\min\\{x,d\\} - r (\\max\\{x-d,0\\} + q) - c \\mathbb{1}\\{q>0\\}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Inventory dynamics model\n",
    "\n",
    "- transition rule is given by  \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "x' &=& \\max\\{x-d,0\\} + q \\\\\n",
    "d' &-& \\text{idiosyncratic}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Parameters in the model:\n",
    "\n",
    "- $ p $ is the profit per one unit of (supplied) good  \n",
    "- $ c $ is the fixed cost of ordering any amount of new inventory  \n",
    "- $ r $ is the cost of storing one unit of good  \n",
    "- $ d $ follows truncated geometric distribution with the support $ i \\in \\{0,1,\\dots,N\\} $ and corresponding probabilities $ pr_i = (1-\\lambda)^i \\lambda $ (with last probability corrected for truncation)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bellman equation\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "V(x,d) &=& \\max_{q \\ge 0} \\Big\\{ \\pi + \\beta \\mathbb{E}\\Big[ V\\big(x', d' \\big) \\Big| x,d,q \\Big] \\Big\\} \\\\\n",
    "&=& \\max_{q \\ge 0} \\Big\\{ sp - x' r - c \\mathbb{1}\\{q>0\\}\n",
    "+ \\beta \\mathbb{E}\\Big[ V\\big( x', d' \\big) \\Big] \\Big\\}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "s &=& \\min\\{x,d\\} \\\\\n",
    "x' &=& \\max\\{x-d,0\\} + q\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "- the expectation is taken over the distribution of the next period demand $ d' $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bellman equation in expected value function space\n",
    "\n",
    "- random demand is *idiosyncratic* = distribution does not depend on the previous period variables  \n",
    "- can reduce the dimensionality of the fixed point problem by rewriting the Bellman operator in expected value function terms  \n",
    "\n",
    "\n",
    "$$\n",
    "EV(x') =  \\mathbb{E}\\Big[ V\\big(x', d' \\big) \\Big| x,d,q \\Big] =  \\mathbb{E}\\Big[ V\\big(x', d' \\big) \\Big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "V(x,d) = \\max_{q \\ge 0} \\Big\\{ sp - x' r - c \\mathbb{1}\\{q>0\\}\n",
    "+ \\beta EV(x') \\Big\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bellman equation in expected value function space\n",
    "\n",
    "Taking the expectation with respect to $ d $ on both sides, we get\n",
    "\n",
    "$$\n",
    "EV(x) = \\mathbb{E}\\Big[ \\max_{q \\ge 0} \\Big\\{ sp - x' r - c \\mathbb{1}\\{q>0\\}\n",
    "+ \\beta EV(x') \\Big\\} \\Big]\n",
    "$$\n",
    "\n",
    "$$\n",
    "EV(x) = \\sum_{d} \\Big[ \\max_{q \\ge 0} \\Big\\{ sp - x' r - c \\mathbb{1}\\{q>0\\}\n",
    "+ \\beta EV(x') \\Big\\} \\Big] pr(d)\n",
    "$$\n",
    "\n",
    "- demand is a discrete random variable with probabilities $ pr(d) $ for different values $ d $  \n",
    "- functional equation in $ EV(\\cdot) $ which is also a contraction mapping!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Policy function\n",
    "\n",
    "- fixed point of the Bellman operator $ EV(x) $ in *expected value function* space defines the solution of the model  \n",
    "- but the policy is still given by the function of two variables  \n",
    "\n",
    "\n",
    "$$\n",
    "q^\\star(x,d) = \\arg\\max_{q \\ge 0} \\Big\\{ sp - x' r - c \\mathbb{1}\\{q>0\\}\n",
    "+ \\beta EV(x') \\Big] \\Big\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "s &=& \\min\\{x,d\\} \\\\\n",
    "x' &=& \\max\\{x-d,0\\} + q\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Further refinements\n",
    "\n",
    "Sequence of events in each time period (timing assumptions in the model):\n",
    "\n",
    "- start period with inventory $ x $  \n",
    "- demand $ d $ is realized  \n",
    "- decision $ q^\\star(x,d) $  \n",
    "- current profit is computed as function of $ x,d,q $  \n",
    "- the next period inventory $ x' =  \\max\\{x-d,0\\} + q $ is stored for the rest of the period  \n",
    "\n",
    "\n",
    "Note that ordering decision is made based on the stock *remaining after trading is over* $ \\max\\{x-d,0\\} $ rather than on $ x $ and $ d $ independently!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Post trade stock (remaining stock)\n",
    "\n",
    "- let $ y = \\max\\{x-d,0\\} = x - \\min\\{x,d\\} $ denote the remaining inventory after the trading phase  \n",
    "- now the optimal policy can be written as  \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "q^\\star(x,d) &=& \\arg\\max_{q \\ge 0} \\Big\\{ sp - x'r - c \\mathbb{1}\\{q>0\\}\n",
    "+ \\beta EV(x') \\Big\\}\\\\\n",
    "q^\\star(x,d) &=& \\arg\\max_{q \\ge 0} \\Big\\{ p \\min\\{x,d\\} - (y + q)r  - c \\mathbb{1}\\{q>0\\}\n",
    "+ \\beta EV(y+q) \\Big\\}\\\\\n",
    "q^\\star(x,d) &=& \\arg\\max_{q \\ge 0} \\Big\\{ - q r - c \\mathbb{1}\\{q>0\\}\n",
    "+ \\beta EV(y+q) \\Big\\} = q^\\star(y)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "- thus, policy function is function of *one* variable $ q^\\star(y) $ which is convenient  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Resulting compact solution for the inventory model\n",
    "\n",
    "- Bellman equation in expected value function space  \n",
    "\n",
    "\n",
    "$$\n",
    "EV(x) = \\sum_{d} \\Big[ p \\min\\{x,d\\} - yr + \\max_{q \\ge 0} \\Big\\{ -qr -c \\mathbb{1}\\{q>0\\}\n",
    "+ \\beta EV(y+q) \\Big\\} \\Big] pr(d)\n",
    "$$\n",
    "\n",
    "- Definition of the policy function, assuming $ EV(x) $ satisfies the equation above  \n",
    "\n",
    "\n",
    "$$\n",
    "q^\\star(y) = \\arg\\max_{q \\ge 0} \\Big\\{ -qr - c \\mathbb{1}\\{q>0\\} + \\beta EV(y+q) \\Big\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\max\\{x-d,0\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Preparing for policy iterations\n",
    "\n",
    "- Policy $ q(y) $ evaluation (in terms of expected value function) is $ EV(x) $ that solves  \n",
    "\n",
    "\n",
    "$$\n",
    "EV(x) = \\sum_{d} \\Big[ p \\min\\{x,d\\} - (y+q(y))r -c \\mathbb{1}\\{q(y)>0\\}\n",
    "+ \\beta EV(y+q(y)) \\Big] pr(d)\n",
    "$$\n",
    "\n",
    "- Improved policy $ q'(y) $ given expected value $ EV(x) $ is  \n",
    "\n",
    "\n",
    "$$\n",
    "q'(y) = \\arg\\max_{q \\ge 0} \\Big\\{ -qr - c \\mathbb{1}\\{q>0\\} + \\beta EV(y+q) \\Big\\}\n",
    "$$\n",
    "\n",
    "Policy iterations solver is applicable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Examine the problem in step 1 (policy evaluation)\n",
    "\n",
    "$$\n",
    "EV(x) = \\sum_{d} \\Big[ p \\min\\{x,d\\} - (y+q(y))r -c \\mathbb{1}\\{q(y)>0\\}\n",
    "+ \\beta EV(y+q(y)) \\Big] pr(d)\n",
    "$$\n",
    "\n",
    "- state space $ S = \\{0,1,\\dots,N\\} $, $ x,d,y,q \\in S $ (all function are defined on finite $ S $)  \n",
    "- $ EV(x) $ is given by a vector $ EV $ with $ N+1 $ elements corresponding to points in $ S $  \n",
    "- $ EV(y+q) $ is a particular indexing of the elements of $ EV $, can be expressed as matrix multiplication  \n",
    "- otherwise RHS is linear function of the re-indexed vector $ EV $  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# example of re-indexing as matrix multiplication\n",
    "import numpy as np\n",
    "ev = np.array([11,21,32,43,54,65,76])  # vector EV\n",
    "j = [1,0,2,2,4,3,5]                    # represent y+q(y)\n",
    "n = ev.size\n",
    "M = np.zeros((n,n),dtype=int)  # start with all zeros\n",
    "M[range(n),j] = 1              # mark\n",
    "ev1 = M@ev\n",
    "print('EV=',ev,'y+q(y)=',j,'re-indexing matrix =',M,'EV(y+q(y))',ev1,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Plan for the implementation\n",
    "\n",
    "- let $ EV $ be the vector of values of $ EV(x) $ for all $ x $  \n",
    "- let $ M(d) $ denote the matrix such that $ M(d) \\cdot EV = EV(y+q(y)) $  \n",
    "- let $ C(d) = p \\min\\{x,d\\} - (y+q(y))r -c \\mathbb{1}\\{q(y)>0\\} $ denote the constant term  \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "EV &=& \\sum_{d} \\Big[ C(d) + \\beta M(d) \\cdot EV \\Big] pr(d) \\\\\n",
    "&=& \\sum_{d} pr(d) C(d)  + \\beta \\sum_{d} pr(d) M(d) \\cdot EV  = \\bar{C} + \\beta \\bar{M} \\cdot EV\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "- where $ \\bar{M} = \\sum_{d} pr(d) M(d) $ is a square matrix  \n",
    "- *linear system* $ (I-\\beta\\bar{M}) \\cdot EV = \\bar{C} $!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "# COPY of the code from exercise 11, only infinite horizon version, combined into single class\n",
    "class inventory_model:\n",
    "    '''Small class to hold model fundamentals and its solution'''\n",
    "\n",
    "    def __init__(self,label='noname',\n",
    "                 max_inventory=10,  # upper bound on the state space\n",
    "                 c = 3.2,            # fixed cost of order\n",
    "                 p = 2.5,           # profit per unit of good\n",
    "                 r = 0.5,           # storage cost per unit of good\n",
    "                 β = 0.95,          # discount factor\n",
    "                 dp = 0.5,          # parameter in geometric distribution of demand\n",
    "                 demand = 4         # fixed demand\n",
    "                 ):\n",
    "        '''Create model with default parameters'''\n",
    "        self.label=label # label for the model instance\n",
    "        self.c, self.p, self.r, self.β, self.dp= c, p, r, β, dp\n",
    "        self.demand = demand\n",
    "        # created dependent attributes (it would be better to have them updated when underlying parameters change)\n",
    "        self.n = max_inventory+1    # number of inventory levels\n",
    "        self.upper = int(max_inventory)  # upper boundary on inventory\n",
    "        self.x = np.arange(self.n,dtype=int)  # all possible values of inventory and demand (state space)\n",
    "\n",
    "    def __repr__(self):\n",
    "        '''String representation of the model'''\n",
    "        return 'Inventory model labeled \"{}\"\\nParamters (c,p,r,β) = ({},{},{},{})\\nDemand={}\\nUpper bound on inventory {}' \\\n",
    "               .format (self.label,self.c,self.p,self.r,self.β,self.demand,self.upper)\n",
    "\n",
    "    def sales(self,x,d):\n",
    "        '''Sales in given period'''\n",
    "        return np.minimum(x,d)\n",
    "\n",
    "    def next_x(self,x,d,q):\n",
    "        '''Inventory to be stored, becomes next period state'''\n",
    "        return x - self.sales(x,d) + q\n",
    "\n",
    "    def profit(self,x,d,q):\n",
    "        '''Profit in given period'''\n",
    "        return self.p * self.sales(x,d) - self.r * self.next_x(x,d,q) - self.c * (q>0)\n",
    "\n",
    "    def demand_pr(self,plot=False):\n",
    "        '''Computes stochastic demand probs'''\n",
    "        k = np.arange(self.n)  # all possible values of demand\n",
    "        pr = (1-self.dp)**k *self.dp\n",
    "        pr[-1] = 1 - pr[:-1].sum()  # update last prob to ensure sum=1\n",
    "        if plot:\n",
    "            plt.step(self.x,pr,where='mid')\n",
    "            plt.title('Distribution of demand')\n",
    "            plt.show()\n",
    "        return pr\n",
    "\n",
    "    def bellman(self,ev0):\n",
    "        '''Bellman operator for inventory model\n",
    "           Inputs: model object\n",
    "                   next period EXPECTED value function\n",
    "        '''\n",
    "        pr = self.demand_pr()\n",
    "        ev1 = np.zeros(shape=self.x.shape)\n",
    "        for j,d in enumerate(self.x):  # over all values of demand\n",
    "            # create the grid of choices (same as x), column-vector\n",
    "            q = self.x[:,np.newaxis]\n",
    "            # compute current period profit (relying on numpy broadcasting to get the matrix with choices in rows)\n",
    "            p = self.profit(self.x,d,q)\n",
    "            # indexes for next period value with extrapolation using last value\n",
    "            i = np.minimum(self.next_x(self.x,d,q),self.upper)\n",
    "            # compute the Bellman maximand\n",
    "            vm = p + self.β*ev0[i]\n",
    "            # find max and argmax\n",
    "            v1 = np.amax(vm,axis=0)  # maximum in every column\n",
    "            ev1 = ev1 + pr[j]*v1\n",
    "        q1 = self.optimal_policy(ev1)\n",
    "        return ev1, q1\n",
    "\n",
    "    def optimal_policy(self,ev):\n",
    "        '''Computes the optimal policy function as function of post trade stock for the stochastic\n",
    "        inventory dynamics model for given EV function'''\n",
    "        # idea: 2-dim array with q in axes 0, y = max(x-d,0) in axis 1\n",
    "        q = self.x[:,np.newaxis]  # choices\n",
    "        y = self.x[np.newaxis,:]  # post trading stock\n",
    "        # indexes for next period value with extrapolation using last value\n",
    "        i = np.minimum(y+q,self.upper)\n",
    "        # compute the Bellman maximand\n",
    "        vm = -self.r*q -self.c*(q>0) + self.β*ev[i]\n",
    "        # find argmax and argmax\n",
    "        return np.argmax(vm,axis=0)  # maximum in every column\n",
    "\n",
    "    def solve_vfi(self,tol=1e-6,maxiter=500,callback=None):\n",
    "        '''Solves the model using value function iterations\n",
    "        '''\n",
    "        ev0 = np.zeros(self.n) # initial point for VFI\n",
    "        for i in range(maxiter):  # main loop\n",
    "            ev1, q1 = self.bellman(ev0)  # update approximation\n",
    "            err = np.amax(np.abs(ev0-ev1))\n",
    "            if callback != None: callback(iter=i,err=err,ev1=ev1,ev0=ev0,q1=q1,model=self)\n",
    "            if err<tol:\n",
    "                break  # break out if converged\n",
    "            ev0 = ev1  # get ready to the next iteration\n",
    "        else:\n",
    "            raise RuntimeError('Failed to converge in %d iterations'%maxiter)\n",
    "        return ev1, q1\n",
    "\n",
    "    def policy_evaluation(self,q):\n",
    "        '''The expected value of the given policy\n",
    "           Returns a vector ev for all points in the state space\n",
    "        '''\n",
    "        pr = self.demand_pr()\n",
    "        Cbar = np.zeros(self.n)\n",
    "        Mbar = np.zeros((self.n,self.n))\n",
    "        for j,d in enumerate(self.x):          # over all values of demand\n",
    "            y = self.x - self.sales(self.x,d)  # post trade stock for all values of x | d\n",
    "            p = self.profit(self.x,d,q[y])     # profits for all values of x | d, q()\n",
    "            Cbar += pr[j]*p\n",
    "            i = np.minimum(y+q[y],self.upper)  # next period stock for all x | d, q()\n",
    "            M = np.zeros((self.n,self.n),dtype=int)  # start with all zeros\n",
    "            M[range(self.n),i] = 1             # transformation matrix\n",
    "            Mbar += pr[j]*M\n",
    "        A = np.eye(self.n) - self.β*Mbar\n",
    "        ev = np.linalg.solve(A,Cbar)\n",
    "        return ev\n",
    "\n",
    "    def solve_policyiter(self,tol=1e-6,maxiter=500,callback=None):\n",
    "        '''Solves the model using policy iterations\n",
    "        '''\n",
    "        q0 = np.zeros(self.n,dtype=int) # initial point for policy iterations\n",
    "        for i in range(maxiter):  # main loop\n",
    "            # step 1: policy evaluation\n",
    "            ev = self.policy_evaluation(q0)\n",
    "            # step 2: policy improvement\n",
    "            q1 = self.optimal_policy(ev)\n",
    "            err = np.amax(np.abs(q0-q1))\n",
    "            if callback != None: callback(iter=i,err=err,ev1=ev,q1=q1,model=self)\n",
    "            if err<tol:\n",
    "                break  # break out if converged\n",
    "            q0 = q1  # get ready to the next iteration\n",
    "        else:\n",
    "            raise RuntimeError('Failed to converge in %d iterations'%maxiter)\n",
    "        ev1 = self.policy_evaluation(q1)\n",
    "        return ev1,q1\n",
    "\n",
    "    def solve_show(self,method='vfi',maxiter=1000,tol=1e-6,**kvargs):\n",
    "        '''Illustrate solution'''\n",
    "        fig1, (ax1,ax2) = plt.subplots(1,2,figsize=(14,8))\n",
    "        ax1.grid(b=True, which='both', color='0.65', linestyle='-')\n",
    "        ax2.grid(b=True, which='both', color='0.65', linestyle='-')\n",
    "        ax1.set_xlabel('Inventory')\n",
    "        ax2.set_xlabel('Post trade inventory')\n",
    "        ax1.set_title('Value function')\n",
    "        ax2.set_title('Optimal new orders')\n",
    "        def callback(**argvars):\n",
    "            mod, ev, q = argvars['model'],argvars['ev1'],argvars['q1']\n",
    "            ax1.step(mod.x,ev,color='k',alpha=0.25,where='mid')\n",
    "            ax2.step(mod.x,q,color='k',alpha=0.25,where='mid')\n",
    "            print(argvars['iter'],end=' ')\n",
    "        if method=='vfi':\n",
    "            ev,pk = self.solve_vfi(maxiter=maxiter,tol=tol,callback=callback,**kvargs)\n",
    "        elif method=='policyiter':\n",
    "            ev,pk = self.solve_policyiter(maxiter=maxiter,tol=tol,callback=callback,**kvargs)\n",
    "        else:\n",
    "            raise RuntimeError('Unknown method in solve_show()')\n",
    "        # add solutions\n",
    "        ax1.step(self.x,ev,color='r',linewidth=2.5,where='mid')\n",
    "        ax2.step(self.x,pk,color='r',linewidth=2.5,where='mid')\n",
    "        plt.show()\n",
    "        return ev,pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mod = inventory_model(max_inventory=25)\n",
    "mod.dp=.25\n",
    "mod.c = .25\n",
    "mod.p = 3.5\n",
    "mod.r = 0.4\n",
    "mod.β = 0.9\n",
    "mod.demand_pr(plot=True)\n",
    "ev1,q1 = mod.solve_vfi()\n",
    "mod.solve_show()\n",
    "ev2,q2 = mod.solve_policyiter()\n",
    "mod.solve_show(method='policyiter');\n",
    "print('Diff in value functions = %1.3e\\nDiff in policy functions = %1.5f'%(np.amax(ev1-ev2),np.amax(q1-q2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def optimal_policy(self,ev):\n",
    "    '''Computes the optimal policy function as function of post trade stock for the stochastic\n",
    "    inventory dynamics model for given EV function'''\n",
    "    # idea: 2-dim array with q in axes 0, y = max(x-d,0) in axis 1\n",
    "    q = self.x[:,np.newaxis]  # choices\n",
    "    y = self.x[np.newaxis,:]  # post trading stock\n",
    "    # indexes for next period value with extrapolation using last value\n",
    "    i = np.minimum(y+q,self.upper)\n",
    "    # compute the Bellman maximand\n",
    "    vm = -self.r*q -self.c*(q>0) + self.β*ev[i]\n",
    "    # find argmax and argmax\n",
    "    return np.argmax(vm,axis=0)  # maximum in every column\n",
    "\n",
    "ev,_ = mod.solve_vfi()\n",
    "q = optimal_policy(mod,ev)\n",
    "print('Optimal orders of new inventory for y = max(x-d,0):')\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# COPY from exercise 11\n",
    "def optimal_policy_old(m,ev):\n",
    "    '''Computes the optimal policy function for the stochastic\n",
    "    inventory dynamics model for given EV function'''\n",
    "    # idea: 3-dim array with q in axes 0, d in axis 1 and x in axis 2\n",
    "    q = m.x[:,np.newaxis,np.newaxis]  # choices\n",
    "    d = m.x[np.newaxis,:,np.newaxis]  # demand\n",
    "    x = m.x[np.newaxis,np.newaxis,:]  # inventories\n",
    "    # compute current period profit (relying on numpy broadcasting to get the matrix with choices in rows)\n",
    "    p = m.profit(x,d,q)  # 3-dim array\n",
    "    # indexes for next period value with extrapolation using last value\n",
    "    i = np.minimum(m.next_x(x,d,q),m.upper)\n",
    "    # compute the Bellman maximand\n",
    "    vm = p + m.β*ev[i]\n",
    "    # find argmax and argmax\n",
    "    return np.argmax(vm,axis=0)  # maximum in every column\n",
    "\n",
    "q = optimal_policy_old(mod,ev)\n",
    "print('Optimal orders of new inventory for d,x:\\n(d in rows, x in columns)')\n",
    "print(q)\n",
    "# Note the symmetry in the optimal policy!\n",
    "# This implies that knowing both x and d is not necessary for the\n",
    "# optional new order, it's enough to condition on the inventory\n",
    "# remaining after sales, i.e. x-min(x,d) = max(0,x-d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Convergence rate comparison between policy iterations and VFI\n",
    "\n",
    "- generally, policy iterations converge much faster  \n",
    "- more work is needed for each iteration (policy evaluation step!)  \n",
    "- if first step done efficiently, significant advantage over VFI  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Modified policy iteration algorithm\n",
    "\n",
    "In problems where the first step (policy evaluation) is hard to perform efficiently, it can be\n",
    "replaced by a finite forward simulation of Bellman operator under current policy to yield an approximate\n",
    "value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further learning resources\n",
    "\n",
    "- Discrete state dynamic programming and policy iterations at QuantEcon\n",
    "  [https://python-advanced.quantecon.org/discrete_dp.html](https://python-advanced.quantecon.org/discrete_dp.html)  \n",
    "- On theoretical properties of Howard’s algorithm\n",
    "  [https://link.springer.com/chapter/10.1007/978-3-642-17517-6_37#:~:text=Howard’s%20policy%20iteration%20algorithm%20is,Markov%20Decision%20Processes%20(MDPs).&text=The%20theoretical%20complexity%20of%20Howard’s,known%20on%20its%20running%20time](https://link.springer.com/chapter/10.1007/978-3-642-17517-6_37#:~:text=Howard's%20policy%20iteration%20algorithm%20is,Markov%20Decision%20Processes%20%28MDPs%29.&text=The%20theoretical%20complexity%20of%20Howard's,known%20on%20its%20running%20time).  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "date": 1605013462.310679,
  "filename": "43_policy_iter.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Foundations of Computational Economics #43"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}