{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Foundations of Computational Economics #38\n",
    "\n",
    "by Fedor Iskhakov, ANU\n",
    "\n",
    "<img src=\"_static/img/dag3logo.png\" style=\"width:256px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Dynamic programming with continuous choice\n",
    "\n",
    "<img src=\"_static/img/lecture.png\" style=\"width:64px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"_static/img/youtube.png\" style=\"width:65px;\">\n",
    "\n",
    "[https://youtu.be/pAEm9cZd92Y](https://youtu.be/pAEm9cZd92Y)\n",
    "\n",
    "Description: Optimization in Python. Consumption-savings model with continuous choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Goal: take continuous choice seriously and deal with it without discretization\n",
    "\n",
    "- no discretization of choice variables  \n",
    "- need to employ numerical optimizer to find optimal continuous choice in Bellman equation  \n",
    "- optimization problem has to be solved for all points in the state space  \n",
    "\n",
    "\n",
    "Implement the continuous version of Bellman operator for the stochastic consumption-savings model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Consumption-savings problem (Deaton model)\n",
    "\n",
    "$$\n",
    "V(M)=\\max_{0 \\le c \\le M}\\big\\{u(c)+\\beta \\mathbb{E}_{y} V\\big(\\underset{=M'}{\\underbrace{R(M-c)+\\tilde{y}}}\\big)\\big\\}\n",
    "$$\n",
    "\n",
    "- discrete time, infinite horizon  \n",
    "- one continuous choice of consumption $ 0 \\le c \\le M $  \n",
    "- state space: consumable resources in the beginning of the period $ M $, discretized  \n",
    "- income $ \\tilde{y} $, follows log-normal distribution with $ \\mu = 0 $ and $ \\sigma $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "V(M)=\\max_{0 \\le c \\le M}\\big\\{u(c)+\\beta \\mathbb{E}_{y} V\\big(\\underset{=M'}{\\underbrace{R(M-c)+\\tilde{y}}}\\big)\\big\\}\n",
    "$$\n",
    "\n",
    "- preferences are given by time separable utility $ u(c) = \\log(c) $  \n",
    "- discount factor $ \\beta $  \n",
    "- gross return on savings $ R $, fixed  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continuous (non-discretized) Bellman equation\n",
    "\n",
    "Have to compute\n",
    "\n",
    "$$\n",
    "\\max_{0 \\le c \\le M}\\big\\{u(c)+\\beta \\mathbb{E}_{y} V\\big(R(M-c)+\\tilde{y}\\big)\\big\\} = \\max_{0 \\le c \\le M} G(M,c)\n",
    "$$\n",
    "\n",
    "using numerical optimization algorithm\n",
    "\n",
    "- constrained optimization (bounds on $ c $)  \n",
    "- have to interpolate value function $ V(\\cdot) $ for every evaluation of objective $ G(c) $  \n",
    "- have to solve this optimization problem for **all possible values** $ M $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Numerical optimization in Python\n",
    "\n",
    "Optimization can be approached\n",
    "\n",
    "1. **directly**, or through the lenses of analytic  \n",
    "1. **first order conditions**, assuming the objective function is differentiable  \n",
    "\n",
    "\n",
    "- FOC approach is equation solving, see video 13, 22, 23  \n",
    "- here focus on optimization itself  \n",
    "\n",
    "\n",
    "The two approaches are equivalent in terms of computational complexity, end even numerically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newton method as optimizer\n",
    "\n",
    "$$\n",
    "\\max_{x \\in \\mathbb{R}} f(x) = -x^4 + 2.5x^2 + x + 2\n",
    "$$\n",
    "\n",
    "Solve the first order condition:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "f'(x)=-4x^3 + 5x +1 &=& 0 \\\\\n",
    "-4x(x^2-1) + x+1 &=& 0 \\\\\n",
    "(x+1)(-4x^2+4x+1) &=& 0 \\\\\n",
    "\\big(x+1\\big)\\big(x-\\frac{1}{2}-\\frac{1}{\\sqrt{2}}\\big)\\big(x-\\frac{1}{2}+\\frac{1}{\\sqrt{2}}\\big) &=& 0\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Taylor series expansion of the equation\n",
    "\n",
    "Let $ x' $ be an approximate solution of the equation $ g(x)=f'(x)=0 $\n",
    "\n",
    "$$\n",
    "g(x') = g(x) + g'(x)(x'-x) + \\dots = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "x' = x - g(x)/g'(x)\n",
    "$$\n",
    "\n",
    "Newton step towards $ x' $ from an approximate solution $ x_i $ at iteration $ i $ is then\n",
    "\n",
    "$$\n",
    "x_{i+1} = x_i - g(x_i)/g'(x_i) = x_i - f'(x_i)/f''(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Or use repeated quadratic approximations\n",
    "\n",
    "Given approximate solution $ x_i $ at iteration $ i $, approximate function $ f(x) $ using first three terms of Taylor series\n",
    "\n",
    "$$\n",
    "\\hat{f}(x) = f(x_i) + f'(x_i) (x-x_i) + \\tfrac{1}{2} f''(x_i) (x-x_i)^2\n",
    "$$\n",
    "\n",
    "The maximum/minimum of this quadratic approximation is given by\n",
    "\n",
    "$$\n",
    "{\\hat{f}}'(x) = f'(x_i) + f''(x_i) (x-x_i) = 0\n",
    "$$\n",
    "\n",
    "Leading to the Newton step\n",
    "\n",
    "$$\n",
    "x = x_{i+1} = x_i - f'(x_i)/f''(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def newton(fun,grad,x0,tol=1e-6,maxiter=100,callback=None):\n",
    "    '''Newton method for solving equation f(x)=0\n",
    "    with given tolerance and number of iterations.\n",
    "    Callback function is invoked at each iteration if given.\n",
    "    '''\n",
    "    for i in range(maxiter):\n",
    "        x1 = x0 - fun(x0)/grad(x0)\n",
    "        err = abs(x1-x0)\n",
    "        if callback != None: callback(err=err,x0=x0,x1=x1,iter=i)\n",
    "        if err<tol: break\n",
    "        x0 = x1\n",
    "    else:\n",
    "        raise RuntimeError('Failed to converge in %d iterations'%maxiter)\n",
    "    return (x0+x1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "F = lambda x: -x**4+2.5*x**2+x+2 # main function\n",
    "f = lambda x: -4*x**3+5*x+1      # FOC\n",
    "g = lambda x: -12*x**2+5         # derivative of FOC\n",
    "\n",
    "# make nice seriest of plots\n",
    "a,b = -1.5,1.5  # upper and lower limits\n",
    "xd = np.linspace(a,b,1000)  # x grid\n",
    "ylim1 = [min(np.amin(f(xd))-1,0),max(np.amax(f(xd))+1,0)]\n",
    "ylim2 = [min(np.amin(F(xd))-1,0),max(np.amax(F(xd))+1,0)]\n",
    "print(ylim1,ylim2)\n",
    "def plot_step(x0,x1,iter,**kwargs):\n",
    "    plot_step.iter = iter+1\n",
    "    if iter<10:\n",
    "        fig1, (ax1,ax2) = plt.subplots(1,2,figsize=(16,6))\n",
    "        ax1.set_title('FOC equation solver')\n",
    "        ax1.plot(xd,f(xd),c='red')  # plot the function\n",
    "        ax1.plot([a,b],[0,0],c='black')  # plot zero line\n",
    "        ax1.plot([x0,x0],ylim1,c='grey') # plot x0\n",
    "        l = lambda z: g(x0)*(z - x1)\n",
    "        ax1.plot(xd,l(xd),c='green')  # plot the function\n",
    "        ax1.set_ylim(bottom=ylim1[0],top=ylim1[1])\n",
    "        ax2.set_title('Optimizer')\n",
    "        ax2.plot(xd,F(xd),c='red')  # plot the function\n",
    "        ax2.plot([x0,x0],ylim2,c='grey') # plot x0\n",
    "        l = lambda z: F(x0)+f(x0)*(z-x0)+(g(x0)*(z-x0)**2)/2\n",
    "        ax2.plot(xd,l(xd),c='green')  # plot the function\n",
    "        ax2.plot([x1,x1],ylim2,c='grey') # plot x1\n",
    "        ax2.set_ylim(bottom=ylim2[0],top=ylim2[1])\n",
    "        ax1.set_ylabel('Iteration %d'%(iter+1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "newton(f,g,x0=-1.3,callback=plot_step) # 0.9, 0.42\n",
    "print('Converged in %d iterations'%plot_step.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multidimensional case\n",
    "\n",
    "$$\n",
    "\\max_{x_1,\\dots,x_n} F(x_1,\\dots,x_n)\n",
    "$$\n",
    "\n",
    "- the Newton optimization method would work with multivariate function $ F(x_1,\\dots,x_n) $, *gradient* vector $ \\nabla F(x_1,\\dots,x_n) $\n",
    "  composed of partial derivatives, and a *Hessian* matrix $ \\nabla^2 F(x_1,\\dots,x_n) $ composed of second order partial derivatives of $ F(x_1,\\dots,x_n) $  \n",
    "- the FOC solver Newton method would work with vector-valued multivariate function $ G(x_1,\\dots,x_n)=\\nabla F(x_1,\\dots,x_n) $,\n",
    "  and a *Jacobian* matrix of first order partial derivatives of all of the outputs of the function $ G(x_1,\\dots,x_n) $ with respect to all arguments  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Newton step in multidimensional case\n",
    "\n",
    "$$\n",
    "x_{i+1} = x_i - \\frac{F'(x_i)}{F''(x_i)} = x_i - \\big( \\nabla^2 F(x_i) \\big)^{-1} \\nabla F(x_i)\n",
    "$$\n",
    "\n",
    "- requires *inverting* the Hessian/Jacobian matrix  \n",
    "- when analytic Hessian/Jacobian is not available, numerical differentiation can be used (yet slow and imprecise)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quasi-Newton methods\n",
    "\n",
    "**SciPy.optimize**\n",
    "\n",
    "Main idea: replace Jacobian/Hessian with approximation. For example,\n",
    "when costly to compute, and/or unavailable in analytic form.\n",
    "\n",
    "- DFP (Davidon–Fletcher–Powell)  \n",
    "- BFGS (Broyden–Fletcher–Goldfarb–Shanno)  \n",
    "- SR1 (Symmetric rank-one)  \n",
    "- BHHH (Berndt–Hall–Hall–Hausman) $ \\leftarrow $ for statistical application and estimation!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Broader view on the optimization methods\n",
    "\n",
    "1. Line search methods  \n",
    "  - Newton and Quasi-Newton  \n",
    "  - Gradient descent  \n",
    "1. Trust region methods  \n",
    "  - Approximation of function in question in a ball around the current point  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Derivative free algorithms  \n",
    "  - Nelder-Mead (simplex)  \n",
    "  - Pattern search  \n",
    "1. Global solution algorithms  \n",
    "  - Simulation based  \n",
    "  - Genetic algorithms  \n",
    "1. **Poly-algorithms** Combinations of other algorithms  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Global convergence of Newton method\n",
    "\n",
    "Newton step: $ x_{i+1} = x_i + s_i $ where $ s_i $ is the *direction* of the step\n",
    "\n",
    "$$\n",
    "s_i = - \\frac{f'(x_i)}{f''(x_i)} = - \\big( \\nabla^2 f(x_i) \\big)^{-1} \\nabla f(x_i)\n",
    "$$\n",
    "\n",
    "Newton method becomes globally convergent with a subproblem of choosing step size $ \\tau $, such that\n",
    "\n",
    "$$\n",
    "x_{i+1} = x_i + \\tau s_i\n",
    "$$\n",
    "\n",
    "**Globally convergent to local optimum**: converges from any starting value, but is not guaranteed to find global optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent\n",
    "\n",
    "$$\n",
    "x_{i+1} = x_i - \\tau \\nabla f(x_i)\n",
    "$$\n",
    "\n",
    "- $ \\nabla f(x_i) $ is direction of the fastest change in the function\n",
    "  value  \n",
    "- As a greedy algorithm, can be much slower that Newton.  \n",
    "- Finding optimal step size $ \\tau $ is a separate one-dimensional optimization sub-problem  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Derivative-free methods\n",
    "\n",
    "**Methods of last resort!**\n",
    "\n",
    "- Grid search (`brute` in SciPy)  \n",
    "- Nelder-Mead (“simplex”)  \n",
    "- Pattern search (generalization of grid search)  \n",
    "- Model specific (POUNDerS for min sum of squares)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nelder-Mead\n",
    "\n",
    "1. Initialize a simplex  \n",
    "1. Update simplex based on function values  \n",
    "  - Increase size of the simplex  \n",
    "  - Reduce size of the simplex  \n",
    "  - Reflect (flip) the simplex  \n",
    "1. Iterate until convergence  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nelder-Mead\n",
    "\n",
    "<img src=\"_static/img/nedlermead.png\" style=\"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Trade-off with derivative free methods\n",
    "\n",
    "Only local convergence. Anybody talking about global convergence with\n",
    "derivative free methods is\n",
    "\n",
    "- either assumes something about the problem (for example, concavity),  \n",
    "- or is prepared to wait forever  \n",
    "\n",
    "\n",
    "“An algorithm converges to the global minimum for any continuous\n",
    "$ f $ if and only if the sequence of points visited by the algorithm\n",
    "is dense in $ \\Omega $.” Torn & Zilinskas book “Global Optimization”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Global and simulation-based methods\n",
    "\n",
    "Coincide with derivative-free methods $ \\Rightarrow $ see above!\n",
    "\n",
    "- Simulated annealing (`basinhopping, dual_annealing` in SciPy.optimize)  \n",
    "- Particle swarms  \n",
    "- Evolutionary algorithms  \n",
    "\n",
    "\n",
    "Better idea: Multi-start + poly-algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constrained optimization\n",
    "\n",
    "Optimization in presence of constraints on the variables of the problem.\n",
    "\n",
    "**SciPy.optimize**\n",
    "\n",
    "- Constrained optimization by linear approximation (COBYLA)  \n",
    "- Sequential Least SQuares Programming (SLSQP)  \n",
    "- Trust region with constraints  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Solving for optimal consumption level in cake eating problem\n",
    "\n",
    "<img src=\"_static/img/cake.png\" style=\"width:128px;\">\n",
    "\n",
    "  \n",
    "- Simple version of consumption-savings problem  \n",
    "- No returns on savings $ R=1 $  \n",
    "- No income $ y=0 $  \n",
    "- What is not eaten in period $ t $ is left for the future $ M_{t+1}=M_t-c_t $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bellman equation\n",
    "\n",
    "$$\n",
    "V(M_{t})=\\max_{0 \\le c_{t} \\le M_t}\\big\\{u(c_{t})+\\beta V(\\underset{=M_{t}-c_{t}}{\\underbrace{M_{t+1}}})\\big\\}\n",
    "$$\n",
    "\n",
    "Attack the optimization problem directly and run the optimizer to solve\n",
    "\n",
    "$$\n",
    "\\max_{0 \\le c \\le M} \\big\\{u(c)+\\beta V_{i-1}(M-c) \\big \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thoughts on appropriate method\n",
    "\n",
    "- For Newton we would need first and second derivatives of $ V_{i-1} $, which is\n",
    "  itself only approximated on a grid, so no go..  \n",
    "- The problem is bounded, so constrained optimization method is needed  \n",
    "- **Bisections** should be considered  \n",
    "- Other derivative free methods?  \n",
    "- Quasi-Newton method with bounds?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bounded optimization in Python\n",
    "\n",
    "*Bounded optimization* is a kind of *constrained optimization* with simple\n",
    "bounds on the variables\n",
    "(like Robust Newton algorithm in video 25)\n",
    "\n",
    "Will use **scipy.optimize.minimize_scalar(method=’bounded’)** which uses the\n",
    "Brent method to find a local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "from scipy.optimize import minimize_scalar\n",
    "%matplotlib inline\n",
    "\n",
    "class cake_continuous():\n",
    "    '''Implementation of the cake eating problem with continuous choices.'''\n",
    "\n",
    "    def __init__(self,beta=.9, Wbar=10, ngrid=50, maxiter_bellman=100,tol_bellman=1e-8):\n",
    "        self.beta = beta    # Discount factor\n",
    "        self.Wbar = Wbar    # Upper bound on cake size\n",
    "        self.ngrid = ngrid  # Number of grid points for the size of cake\n",
    "        self.epsilon = np.finfo(float).eps # smallest positive float number\n",
    "        self.grid_state = np.linspace(self.epsilon,Wbar,ngrid) # grid for state space\n",
    "        self.maxiter_bellman = maxiter_bellman # maximum iterations in Bellman solver\n",
    "        self.tol_bellman = tol_bellman # tolerance in Bellman solver\n",
    "\n",
    "    def bellman(self,V0):\n",
    "        #Bellman operator, V0 is one-dim vector of values on grid\n",
    "\n",
    "        def maximand(c,M,interf):\n",
    "            '''Maximand of the Bellman equation'''\n",
    "            Vnext = interf(M-c) # next period value at the size of cake in the next period\n",
    "            V1 = np.log(c) + self.beta*Vnext\n",
    "            return -V1 # negative because of minimization\n",
    "\n",
    "        def findC(M,maximand,interf):\n",
    "            '''Solves for optimal consumption for given cake size M and value function VF'''\n",
    "            opt = {'maxiter':self.maxiter_bellman, 'xatol':self.tol_bellman}\n",
    "            res = minimize_scalar(maximand,args=(M,interf),method='Bounded',bounds=[self.epsilon,M],options=opt)\n",
    "            if res.success:\n",
    "                return res.x # if converged successfully\n",
    "            else:\n",
    "                return M/2 # return some visibly wrong value\n",
    "\n",
    "        # interpolation function for the current approximation of the vaulue function\n",
    "        interfunc = interpolate.interp1d(self.grid_state,V0,kind='slinear',fill_value=\"extrapolate\")\n",
    "        # allocate space for the policy function\n",
    "        c1=np.empty(self.ngrid,dtype='float')\n",
    "        c1[0] = self.grid_state[0]/2 # skip the zero/eps point\n",
    "        # loop over state space\n",
    "        for i in range(1,self.ngrid):\n",
    "            # find optimal consumption level for each point in the state space\n",
    "            c1[i] = findC(self.grid_state[i],maximand,interfunc)\n",
    "        # compute the value function corresponding to the computed policy\n",
    "        V1 = - maximand(c1,self.grid_state,interfunc) # don't forget the negation!\n",
    "        return V1, c1\n",
    "\n",
    "    def solve(self, maxiter=1000, tol=1e-4, callback=None):\n",
    "        '''Solves the model using successive approximations'''\n",
    "        V0=np.log(self.grid_state) # on first iteration assume consuming everything\n",
    "        for iter in range(maxiter):\n",
    "            V1,c1=self.bellman(V0)\n",
    "            if callback: callback(iter,self.grid_state,V1,c1) # callback for making plots\n",
    "            if np.all(abs(V1-V0) < tol):\n",
    "                break\n",
    "            V0=V1\n",
    "        else:  # when i went up to maxiter\n",
    "            print('No convergence: maximum number of iterations achieved!')\n",
    "        return V1,c1\n",
    "\n",
    "    def solve_plot(self, maxiter=1000, tol=1e-4):\n",
    "        '''Illustrate solution'''\n",
    "        fig1, (ax1,ax2) = plt.subplots(1,2,figsize=(14,8))\n",
    "        ax1.grid(b=True, which='both', color='0.65', linestyle='-')\n",
    "        ax2.grid(b=True, which='both', color='0.65', linestyle='-')\n",
    "        ax1.set_title('Value function convergence with VFI')\n",
    "        ax2.set_title('Policy function convergence with VFI')\n",
    "        ax1.set_xlabel('Cake size, W')\n",
    "        ax2.set_xlabel('Cake size, W')\n",
    "        ax1.set_ylabel('Value function')\n",
    "        ax2.set_ylabel('Policy function')\n",
    "        print('Iterations:',end=' ')\n",
    "        def callback(iter,grid,v,c):\n",
    "            print(iter,end=' ') # print iteration number\n",
    "            ax1.plot(grid[1:],v[1:],color='k',alpha=0.25)\n",
    "            ax2.plot(grid,c,color='k',alpha=0.25)\n",
    "        V,c = self.solve(maxiter=maxiter,tol=tol,callback=callback)\n",
    "        # add solutions\n",
    "        ax1.plot(self.grid_state[1:],V[1:],color='r',linewidth=2.5)\n",
    "        ax2.plot(self.grid_state,c,color='r',linewidth=2.5)\n",
    "        plt.show()\n",
    "        return V,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m3 = cake_continuous (beta=0.92,Wbar=10,ngrid=10,tol_bellman=1e-8)\n",
    "V3,c3 = m3.solve_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m3 = cake_continuous (beta=0.92,Wbar=10,ngrid=100,tol_bellman=1e-4)\n",
    "V3,c3 = m3.solve_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Dealing with continuous choice directly using numerical optimization:\n",
    "\n",
    "- is **slow**, consider using lower level language or just in time complication in Python  \n",
    "- more precise, but not ideal, requires additional technical parameters (tolerance and maxiter for within Bellman optimization)  \n",
    "\n",
    "\n",
    "(Will come back to full blown stochastic consumption-savings model in the next practical video.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Further learning resources\n",
    "\n",
    "- Overview of SciPy optimize\n",
    "  [https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html](https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html)  \n",
    "- Docs [https://docs.scipy.org/doc/scipy/reference/optimize.html#module-scipy.optimize](https://docs.scipy.org/doc/scipy/reference/optimize.html#module-scipy.optimize)  \n",
    "- Visualization of Nelder-Mead [https://www.youtube.com/watch?v=j2gcuRVbwR0](https://www.youtube.com/watch?v=j2gcuRVbwR0)  \n",
    "- Brent’s method explained [https://www.youtube.com/watch?v=-bLSRiokgFk](https://www.youtube.com/watch?v=-bLSRiokgFk)  \n",
    "- Many visualizations of Newton and other methods [https://www.youtube.com/user/oscarsveliz/videos](https://www.youtube.com/user/oscarsveliz/videos)  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "date": 1605013462.1790879,
  "filename": "38_optimization.rst",
  "kernelspec": {
   "display_name": "Python",
   "language": "python3",
   "name": "python3"
  },
  "title": "Foundations of Computational Economics #38"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}